{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee6b8018",
   "metadata": {},
   "source": [
    "# **Маркетинг: определение потенциального покупателя**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2541179",
   "metadata": {},
   "source": [
    "# TODO  сдать до 9 марта--------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39646a88",
   "metadata": {},
   "source": [
    "# **1. Описание проекта**\n",
    "\n",
    "Интернет-магазин собирает историю покупателей, проводит рассылки предложений и планирует будущие продажи. \n",
    "\n",
    "Для оптимизации процессов надо выделить пользователей, которые готовы совершить покупку в ближайшее время."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541ae5ca",
   "metadata": {},
   "source": [
    "## **1.1. Цель**\n",
    "Предсказать вероятность покупки в течение 90 дней\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75567c0e",
   "metadata": {},
   "source": [
    "## **1.2. Задачи**\n",
    "\n",
    "● Изучить данные\n",
    "\n",
    "● Разработать полезные признаки\n",
    "\n",
    "● Создать модель для классификации пользователей\n",
    "\n",
    "● Улучшить модель и максимизировать метрику roc_auc\n",
    "\n",
    "● Выполнить тестирование"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1813bebf",
   "metadata": {},
   "source": [
    "## **1.3. Описание данных**\n",
    "\n",
    "### **apparel-purchases** - история покупок\n",
    "\n",
    "Данные о покупках клиентов по дням и по товарам. В каждой записи покупка\n",
    "определенного товара, его цена, количество штук.\n",
    "В таблице есть списки идентификаторов, к каким категориям относится товар. Часто\n",
    "это вложенные категории (например автотовары-аксессуары-освежители), но также может\n",
    "включать в начале списка маркер распродажи или маркер женщинам/мужчинам.\n",
    "Нумерация категорий сквозная для всех уровней, то есть 44 на второй позиции списка\n",
    "или на третьей – это одна и та же категория. Иногда дерево категорий обновляется, поэтому\n",
    "могут меняться вложенности, например ['4', '28', '44', '1594'] или ['4', '44', '1594']. Как\n",
    "обработать такие случаи – можете предлагать свои варианты решения.\n",
    "\n",
    "\n",
    "- `client_id` - идентификатор пользователя\n",
    "- `quantity` - количество товаров в заказе\n",
    "- `price` - цена товара\n",
    "- `category_ids` - вложенные категории, к которым отнсится товар\n",
    "- `date` дата - покупки\n",
    "- `message_id` - идентификатор сообщения из рассылки\n",
    "\n",
    "### **apparel-messages** - история рекламных рассылок\n",
    "\n",
    "Рассылки, которые были отправлены клиентам из таблицы покупок.\n",
    "\n",
    "\n",
    "- `bulk_campaign_id` - идентификатор рекламной кампании (рассылки)\n",
    "- `client_id` - идентификатор пользователя\n",
    "- `message_id` - идентификатор сообщений\n",
    "- `event` - тип действия с сообщением (отправлено, открыто, покупка…)\n",
    "- `channel` - канал рассылки\n",
    "- `date` - дата рассылки\n",
    "- `created_at` - точное время и дата создания сообщения\n",
    "\n",
    "\n",
    "### **apparel-target_binary** - покупка в течение следующих 90 дней\n",
    "\n",
    "\n",
    "- `client_id` - идентификатор пользователя\n",
    "- `target` - целевой признак - клиент совершил покупку в целевом периоде\n",
    "\n",
    "### **full_campaign_daily_event** - агрегация общей базы рассылок по дням и типам событий\n",
    "\n",
    "- `date` - дата\n",
    "- `bulk_campaign_id` - идентификатор рассылки\n",
    "- `count_event` - общее количество каждого события event (все типы событий event)\n",
    "- `nunique_event` - количество уникальных client_id в каждом событии (все типы событий event)\n",
    "\n",
    "\n",
    "### **full_campaign_daily_event_channel** - агрегация по дням с учетом событий и каналов рассылки\n",
    "\n",
    "- `date` - дата\n",
    "- `bulk_campaign_id` - идентификатор рассылки\n",
    "- `count_event` - общее количество каждого события по каналам *\n",
    "- `nunique_event` - количество уникальных client_id по событиям и каналам *\n",
    "\n",
    "* в именах колонок есть все типы событий event и каналов рассылки channel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d81d53",
   "metadata": {},
   "source": [
    "нельзя суммировать по колонкам nunique, потому что это уникальные клиенты в пределах дня, нет данных, повторяются ли они в другие дни."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573f8563",
   "metadata": {},
   "source": [
    "## **1.4. Требования к оформлению**\n",
    "\n",
    "Репозиторий на гитхабе:\n",
    "\n",
    "- тетрадь jupyter notebook с описанием, подготовкой признаков, обучением модели и тестированием\n",
    "- описание проекта и инструкция по использованию в файле README.md\n",
    "- список зависимостей в файле requirements.txt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11967ed",
   "metadata": {},
   "source": [
    "## **1.5. Стэк**\n",
    "\n",
    "● python\n",
    "● pandas\n",
    "● sklearn\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cf98b2",
   "metadata": {},
   "source": [
    "# **2. Настройка рабочего пространства**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3883c73b",
   "metadata": {},
   "source": [
    "## **2.1. Импорт библиотек и настройка рабочего пространства.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b961acc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML \n",
    "import warnings\n",
    "\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from typing import List, Any, Callable, Dict, Optional, Union\n",
    "\n",
    "from phik import phik_matrix\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8113b8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore') # чтобы не было красный полей с предупреждениями об устаревших библиотеках\n",
    "# %matplotlib inline\n",
    "plt.ion() # принудительное отображение графиков matplotlib в VS Code\n",
    "pd.set_option(\"display.max_columns\", None) # чтобы сам df был пошире\n",
    "pd.set_option('display.max_colwidth', None) # чтобы df колонки были пошире\n",
    "pd.set_option('display.float_format', '{:.3f}'.format) # округление чисел в df, чтобы числа не печатал экспоненциально\n",
    "pd.options.display.expand_frame_repr = False # для принта чтобы колонки не переносил рабоатет тольок в vs code, in jupyter notebook получается каша"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f63765",
   "metadata": {},
   "source": [
    "## **2.2. Загрузка данных**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8af65486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данные загружены с домашнего компьютера\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    messages_df = pd.read_csv('datasets/apparel-messages.csv')\n",
    "    purchases_df = pd.read_csv('datasets/apparel-purchases.csv')\n",
    "    target_df = pd.read_csv('datasets/apparel-target_binary.csv')\n",
    "    event_cahnnel_df = pd.read_csv('datasets/full_campaign_daily_event_channel.csv')\n",
    "    event_df = pd.read_csv('datasets/full_campaign_daily_event.csv')\n",
    "    print(\"Данные загружены с домашнего компьютера\")\n",
    "except (FileNotFoundError, OSError):\n",
    "    # Альтернативный путь для запуска из Интернета\n",
    "    messages_df = pd.read_csv('datasets/apparel-messages.csv')\n",
    "    purchases_df = pd.read_csv('datasets/apparel-purchases.csv')\n",
    "    target_df = pd.read_csv('datasets/apparel-target_binary.csv')\n",
    "    event_cahnnel_df = pd.read_csv('datasets/full_campaign_daily_event_channel.csv')\n",
    "    event_df = pd.read_csv('datasets/full_campaign_daily_event.csv')\n",
    "    print(\"Данные загружены из Интернета\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8d64e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bulk_campaign_id</th>\n",
       "      <th>client_id</th>\n",
       "      <th>message_id</th>\n",
       "      <th>event</th>\n",
       "      <th>channel</th>\n",
       "      <th>date</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4439</td>\n",
       "      <td>1515915625626736623</td>\n",
       "      <td>1515915625626736623-4439-6283415ac07ea</td>\n",
       "      <td>open</td>\n",
       "      <td>email</td>\n",
       "      <td>2022-05-19</td>\n",
       "      <td>2022-05-19 00:14:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4439</td>\n",
       "      <td>1515915625490086521</td>\n",
       "      <td>1515915625490086521-4439-62834150016dd</td>\n",
       "      <td>open</td>\n",
       "      <td>email</td>\n",
       "      <td>2022-05-19</td>\n",
       "      <td>2022-05-19 00:39:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4439</td>\n",
       "      <td>1515915625553578558</td>\n",
       "      <td>1515915625553578558-4439-6283415b36b4f</td>\n",
       "      <td>open</td>\n",
       "      <td>email</td>\n",
       "      <td>2022-05-19</td>\n",
       "      <td>2022-05-19 00:51:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4439</td>\n",
       "      <td>1515915625553578558</td>\n",
       "      <td>1515915625553578558-4439-6283415b36b4f</td>\n",
       "      <td>click</td>\n",
       "      <td>email</td>\n",
       "      <td>2022-05-19</td>\n",
       "      <td>2022-05-19 00:52:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4439</td>\n",
       "      <td>1515915625471518311</td>\n",
       "      <td>1515915625471518311-4439-628341570c133</td>\n",
       "      <td>open</td>\n",
       "      <td>email</td>\n",
       "      <td>2022-05-19</td>\n",
       "      <td>2022-05-19 00:56:52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bulk_campaign_id            client_id                              message_id  event channel        date           created_at\n",
       "0              4439  1515915625626736623  1515915625626736623-4439-6283415ac07ea   open   email  2022-05-19  2022-05-19 00:14:20\n",
       "1              4439  1515915625490086521  1515915625490086521-4439-62834150016dd   open   email  2022-05-19  2022-05-19 00:39:34\n",
       "2              4439  1515915625553578558  1515915625553578558-4439-6283415b36b4f   open   email  2022-05-19  2022-05-19 00:51:49\n",
       "3              4439  1515915625553578558  1515915625553578558-4439-6283415b36b4f  click   email  2022-05-19  2022-05-19 00:52:20\n",
       "4              4439  1515915625471518311  1515915625471518311-4439-628341570c133   open   email  2022-05-19  2022-05-19 00:56:52"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client_id</th>\n",
       "      <th>quantity</th>\n",
       "      <th>price</th>\n",
       "      <th>category_ids</th>\n",
       "      <th>date</th>\n",
       "      <th>message_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1515915625468169594</td>\n",
       "      <td>1</td>\n",
       "      <td>1999.000</td>\n",
       "      <td>['4', '28', '57', '431']</td>\n",
       "      <td>2022-05-16</td>\n",
       "      <td>1515915625468169594-4301-627b661e9736d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1515915625468169594</td>\n",
       "      <td>1</td>\n",
       "      <td>2499.000</td>\n",
       "      <td>['4', '28', '57', '431']</td>\n",
       "      <td>2022-05-16</td>\n",
       "      <td>1515915625468169594-4301-627b661e9736d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1515915625471138230</td>\n",
       "      <td>1</td>\n",
       "      <td>6499.000</td>\n",
       "      <td>['4', '28', '57', '431']</td>\n",
       "      <td>2022-05-16</td>\n",
       "      <td>1515915625471138230-4437-6282242f27843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1515915625471138230</td>\n",
       "      <td>1</td>\n",
       "      <td>4999.000</td>\n",
       "      <td>['4', '28', '244', '432']</td>\n",
       "      <td>2022-05-16</td>\n",
       "      <td>1515915625471138230-4437-6282242f27843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1515915625471138230</td>\n",
       "      <td>1</td>\n",
       "      <td>4999.000</td>\n",
       "      <td>['4', '28', '49', '413']</td>\n",
       "      <td>2022-05-16</td>\n",
       "      <td>1515915625471138230-4437-6282242f27843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             client_id  quantity    price               category_ids        date                              message_id\n",
       "0  1515915625468169594         1 1999.000   ['4', '28', '57', '431']  2022-05-16  1515915625468169594-4301-627b661e9736d\n",
       "1  1515915625468169594         1 2499.000   ['4', '28', '57', '431']  2022-05-16  1515915625468169594-4301-627b661e9736d\n",
       "2  1515915625471138230         1 6499.000   ['4', '28', '57', '431']  2022-05-16  1515915625471138230-4437-6282242f27843\n",
       "3  1515915625471138230         1 4999.000  ['4', '28', '244', '432']  2022-05-16  1515915625471138230-4437-6282242f27843\n",
       "4  1515915625471138230         1 4999.000   ['4', '28', '49', '413']  2022-05-16  1515915625471138230-4437-6282242f27843"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1515915625468060902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1515915625468061003</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1515915625468061099</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1515915625468061100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1515915625468061170</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             client_id  target\n",
       "0  1515915625468060902       0\n",
       "1  1515915625468061003       1\n",
       "2  1515915625468061099       0\n",
       "3  1515915625468061100       0\n",
       "4  1515915625468061170       0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>bulk_campaign_id</th>\n",
       "      <th>count_click_email</th>\n",
       "      <th>count_click_mobile_push</th>\n",
       "      <th>count_open_email</th>\n",
       "      <th>count_open_mobile_push</th>\n",
       "      <th>count_purchase_email</th>\n",
       "      <th>count_purchase_mobile_push</th>\n",
       "      <th>count_soft_bounce_email</th>\n",
       "      <th>count_subscribe_email</th>\n",
       "      <th>count_unsubscribe_email</th>\n",
       "      <th>nunique_click_email</th>\n",
       "      <th>nunique_click_mobile_push</th>\n",
       "      <th>nunique_open_email</th>\n",
       "      <th>nunique_open_mobile_push</th>\n",
       "      <th>nunique_purchase_email</th>\n",
       "      <th>nunique_purchase_mobile_push</th>\n",
       "      <th>nunique_soft_bounce_email</th>\n",
       "      <th>nunique_subscribe_email</th>\n",
       "      <th>nunique_unsubscribe_email</th>\n",
       "      <th>count_hard_bounce_mobile_push</th>\n",
       "      <th>count_send_mobile_push</th>\n",
       "      <th>nunique_hard_bounce_mobile_push</th>\n",
       "      <th>nunique_send_mobile_push</th>\n",
       "      <th>count_hard_bounce_email</th>\n",
       "      <th>count_hbq_spam_email</th>\n",
       "      <th>count_send_email</th>\n",
       "      <th>nunique_hard_bounce_email</th>\n",
       "      <th>nunique_hbq_spam_email</th>\n",
       "      <th>nunique_send_email</th>\n",
       "      <th>count_soft_bounce_mobile_push</th>\n",
       "      <th>nunique_soft_bounce_mobile_push</th>\n",
       "      <th>count_complain_email</th>\n",
       "      <th>nunique_complain_email</th>\n",
       "      <th>count_close_mobile_push</th>\n",
       "      <th>nunique_close_mobile_push</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-05-19</td>\n",
       "      <td>563</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-05-19</td>\n",
       "      <td>577</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-05-19</td>\n",
       "      <td>622</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-05-19</td>\n",
       "      <td>634</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-05-19</td>\n",
       "      <td>676</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  bulk_campaign_id  count_click_email  count_click_mobile_push  count_open_email  count_open_mobile_push  count_purchase_email  count_purchase_mobile_push  count_soft_bounce_email  count_subscribe_email  count_unsubscribe_email  nunique_click_email  nunique_click_mobile_push  nunique_open_email  nunique_open_mobile_push  nunique_purchase_email  nunique_purchase_mobile_push  nunique_soft_bounce_email  nunique_subscribe_email  nunique_unsubscribe_email  count_hard_bounce_mobile_push  count_send_mobile_push  nunique_hard_bounce_mobile_push  nunique_send_mobile_push  count_hard_bounce_email  count_hbq_spam_email  count_send_email  nunique_hard_bounce_email  nunique_hbq_spam_email  nunique_send_email  count_soft_bounce_mobile_push  nunique_soft_bounce_mobile_push  count_complain_email  nunique_complain_email  count_close_mobile_push  nunique_close_mobile_push\n",
       "0  2022-05-19               563                  0                        0                 4                       0                     0                           0                        0                      0                        0                    0                          0                   4                         0                       0                             0                          0                        0                          0                              0                       0                                0                         0                        0                     0                 0                          0                       0                   0                              0                                0                     0                       0                        0                          0\n",
       "1  2022-05-19               577                  0                        0                 1                       0                     0                           0                        0                      0                        0                    0                          0                   1                         0                       0                             0                          0                        0                          0                              0                       0                                0                         0                        0                     0                 0                          0                       0                   0                              0                                0                     0                       0                        0                          0\n",
       "2  2022-05-19               622                  0                        0                 2                       0                     0                           0                        0                      0                        0                    0                          0                   2                         0                       0                             0                          0                        0                          0                              0                       0                                0                         0                        0                     0                 0                          0                       0                   0                              0                                0                     0                       0                        0                          0\n",
       "3  2022-05-19               634                  0                        0                 1                       0                     0                           0                        0                      0                        0                    0                          0                   1                         0                       0                             0                          0                        0                          0                              0                       0                                0                         0                        0                     0                 0                          0                       0                   0                              0                                0                     0                       0                        0                          0\n",
       "4  2022-05-19               676                  0                        0                 1                       0                     0                           0                        0                      0                        0                    0                          0                   1                         0                       0                             0                          0                        0                          0                              0                       0                                0                         0                        0                     0                 0                          0                       0                   0                              0                                0                     0                       0                        0                          0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>bulk_campaign_id</th>\n",
       "      <th>count_click</th>\n",
       "      <th>count_complain</th>\n",
       "      <th>count_hard_bounce</th>\n",
       "      <th>count_open</th>\n",
       "      <th>count_purchase</th>\n",
       "      <th>count_send</th>\n",
       "      <th>count_soft_bounce</th>\n",
       "      <th>count_subscribe</th>\n",
       "      <th>count_unsubscribe</th>\n",
       "      <th>nunique_click</th>\n",
       "      <th>nunique_complain</th>\n",
       "      <th>nunique_hard_bounce</th>\n",
       "      <th>nunique_open</th>\n",
       "      <th>nunique_purchase</th>\n",
       "      <th>nunique_send</th>\n",
       "      <th>nunique_soft_bounce</th>\n",
       "      <th>nunique_subscribe</th>\n",
       "      <th>nunique_unsubscribe</th>\n",
       "      <th>count_hbq_spam</th>\n",
       "      <th>nunique_hbq_spam</th>\n",
       "      <th>count_close</th>\n",
       "      <th>nunique_close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-05-19</td>\n",
       "      <td>563</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-05-19</td>\n",
       "      <td>577</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-05-19</td>\n",
       "      <td>622</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-05-19</td>\n",
       "      <td>634</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-05-19</td>\n",
       "      <td>676</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  bulk_campaign_id  count_click  count_complain  count_hard_bounce  count_open  count_purchase  count_send  count_soft_bounce  count_subscribe  count_unsubscribe  nunique_click  nunique_complain  nunique_hard_bounce  nunique_open  nunique_purchase  nunique_send  nunique_soft_bounce  nunique_subscribe  nunique_unsubscribe  count_hbq_spam  nunique_hbq_spam  count_close  nunique_close\n",
       "0  2022-05-19               563            0               0                  0           4               0           0                  0                0                  0              0                 0                    0             4                 0             0                    0                  0                    0               0                 0            0              0\n",
       "1  2022-05-19               577            0               0                  0           1               0           0                  0                0                  0              0                 0                    0             1                 0             0                    0                  0                    0               0                 0            0              0\n",
       "2  2022-05-19               622            0               0                  0           2               0           0                  0                0                  0              0                 0                    0             2                 0             0                    0                  0                    0               0                 0            0              0\n",
       "3  2022-05-19               634            0               0                  0           1               0           0                  0                0                  0              0                 0                    0             1                 0             0                    0                  0                    0               0                 0            0              0\n",
       "4  2022-05-19               676            0               0                  0           1               0           0                  0                0                  0              0                 0                    0             1                 0             0                    0                  0                    0               0                 0            0              0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_list = [messages_df, purchases_df, target_df, event_cahnnel_df, event_df]\n",
    "for df in df_list:\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cc952c",
   "metadata": {},
   "source": [
    "- 3) Для того, чтобы **выполнить предобработку**, необходимо сначала отделить тестовую выборку;\n",
    "- 2) Для того, чтобы  **отделить тестовую выборку**, необходимо объединить разрозненные таблицы;\n",
    "- 1) Для того, чтобы  **объединить таблицы**, необходимо понять как это сделать так, чтоб объединение произошло корректно."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f6e49a",
   "metadata": {},
   "source": [
    "Для начала обратим внимание, что наш целевой признак (совершил покупку или нет) в `apparel-target_binary` == 1/0 для каждого `client_id`, а каждый `client_id` уникальный в `target_df` (одна строка == один клиент и значение 1/0 к нему), а значит, надо другие исходные таблицы перед объединением группировать по `client_id`, чтобы там `client_id` оставался уникальным.\n",
    "\n",
    "Поэтому мы предлагаем следующую тактику объединения:\n",
    "1) Агрегировать покупки по client_id: сколько покупок, суммарный чек, сколько разных категорий, давность последней покупки и т.п.\n",
    "2) Агрегировать сообщения по client_id: сколько сообщений всего, сколько по каждому каналу, сколько открытий, сколько кликов и т.д.\n",
    "Получить две таблицы с одной строкой на client_id, например purchases_agg и messages_agg.\n",
    "3) Сделать финальный merge на уровне клиента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bcd466",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchases_agg = purchases_df.groupby('client_id').agg(\n",
    "    total_purchases=('quantity', 'sum'),\n",
    "    total_amount=('price', 'sum'),\n",
    "    total_categories=('category_ids', 'nunique'),\n",
    "    last_purchase_date=('date', 'max')\n",
    ").reset_index()\n",
    "\n",
    "messages_agg = messages_df.groupby('client_id').agg(\n",
    "    total_messages=('message_id', 'count'),\n",
    "    total_opens=('event', lambda x: (x == 'open').sum()),\n",
    "    total_clicks=('event', lambda x: (x == 'click').sum()),\n",
    "    total_purchases=('event', lambda x: (x == 'purchase').sum())\n",
    ").reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b659e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "   X = (purchases_agg\n",
    "        .merge(messages_agg, on='client_id', how='outer')\n",
    "        .merge(target_df,   on='client_id', how='inner'))  # обычно inner по таргету"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11065da3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0efd864",
   "metadata": {},
   "source": [
    "## **2.3. Сокращение размерности**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b82fd31",
   "metadata": {},
   "source": [
    "Для ускорения предварительной обработки и обучения сократим датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90884329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[:100_000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d571249d",
   "metadata": {},
   "source": [
    "## **2.4. Словарь датафреймов для итераций**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590dde5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {\n",
    "            'train_df': train_df, \n",
    "            'test_df': test_df, \n",
    "            'val_df': val_df\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6657ae2",
   "metadata": {},
   "source": [
    "## **2.5. Деление данных на выборки**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30332a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделение всего датафрейма\n",
    "train_df, test_df = train_test_split(df, test_size=0.25, random_state=42)\n",
    "\n",
    "# Train/Val split\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "total = len(df)\n",
    "\n",
    "print(f'Train: {train_df.shape[0]:>6} строк ({train_df.shape[0]/total*100:>5.1f}%)')\n",
    "print(f'Valid: {val_df.shape[0]:>6} строк ({val_df.shape[0]/total*100:>5.1f}%)')\n",
    "print(f'Test:  {test_df.shape[0]:>6} строк ({test_df.shape[0]/total*100:>5.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cad1957",
   "metadata": {},
   "source": [
    "Для удобства итераций и предобработки создадим словарь датафреймов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b454f1",
   "metadata": {},
   "source": [
    "## **2.6. Переименуем столбцы**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5a9370",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in df_dict.values():\n",
    "    df.columns = [re.sub(r'(?<!^)(?=[A-Z])', '_', col).lower().replace(' ', '_').replace('-', '_') for col in df.columns]\n",
    "    df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb4df1d",
   "metadata": {},
   "source": [
    "# **3. EDA: исследовательский анализ данных**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd84ed2",
   "metadata": {},
   "source": [
    "# **3.1. Оценка качества представленных данных**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ad207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d099fbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c1818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7355655e",
   "metadata": {},
   "source": [
    "## **3.2. Вывод о качестве данных**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecb9139",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11213372",
   "metadata": {},
   "source": [
    "## **3.3. Создание pipeline для предобработки**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb21ff0",
   "metadata": {},
   "source": [
    "Произведем предобработку данных на классах в PipeLine. Для этого создадим следующие классы:\n",
    "\n",
    "- `DecimalPointChanger` - проверяет каждое значение столбца на наличие правильного разделителя дроби, в случае если будет найдена запятая - заменит ее на точку;\n",
    "- `OutlierRemover` - удалит выбросы;\n",
    "- `ImplicitDuplicatesViewer` - отобразит список уникальных нечисловых значений каждого столбца, что должно помочь опредлелить неявные дубликаты в столбцах;\n",
    "- `DuplicateRemover` - удалит явные дубликаты;\n",
    "- `MissingValueHandler` - обрабатывает пропуски на основе выбранной стратегии, по умолчанию, удаляет всю строку, если есть в ней пропуск;\n",
    "- `ColumnRemover` - удаляет лишний столбцы из датафрейма;\n",
    "- `FloatToIntChanger` - преобразует дробное число в целочисленное.\n",
    "\n",
    "Эти классы мы передадим в класс `EDAPreprocessor`, который станет основной состаляющей пайплайна EDA_Preprocessor_pipline, который будет производить предобработку данных. Зпуск пайплайна буддет вызываться функией  `run_preprocessor()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cee4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistakeCorrector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Класс для исправления ошибок в данных.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            columns: List[str], \n",
    "            values_dict: Optional[Dict[Any, Any]] = None, \n",
    "            func: Optional[Callable] = None,\n",
    "            strategy: str = 'auto',\n",
    "            skip_on_test: bool = False\n",
    "            ):\n",
    "        \n",
    "        if not columns:\n",
    "            raise ValueError(\"Параметр 'columns' не может быть пустым\")\n",
    "            \n",
    "        if strategy not in ['dict', 'func', 'auto']:\n",
    "            raise ValueError(\"strategy должен быть 'dict', 'func' или 'auto'\")\n",
    "\n",
    "        self.values_dict = values_dict or {}\n",
    "        self.columns = columns\n",
    "        self.func = func\n",
    "        self.strategy = strategy\n",
    "        self.skip_on_test = skip_on_test\n",
    "        self.fill_values = {}\n",
    "    \n",
    "    def fit(self, X: pd.DataFrame, y=None, **kwargs):\n",
    "        \"\"\"Вычисляет значения для замены на train\"\"\"\n",
    "        if self.strategy == 'auto':\n",
    "            for col in self.columns:\n",
    "                for invalid_val, method in self.values_dict.items():\n",
    "                    # Если метод - это статистика\n",
    "                    if method in ['median', 'mean', 'mode']:\n",
    "                        valid_data = X[col][X[col] != invalid_val]\n",
    "                        \n",
    "                        if method == 'median':\n",
    "                            self.fill_values[(col, invalid_val)] = valid_data.median()\n",
    "                        elif method == 'mean':\n",
    "                            self.fill_values[(col, invalid_val)] = valid_data.mean()\n",
    "                        elif method == 'mode':\n",
    "                            self.fill_values[(col, invalid_val)] = valid_data.mode()[0]\n",
    "                    else:\n",
    "                        # Если метод - это конкретное значение (например, 'petrol')\n",
    "                        self.fill_values[(col, invalid_val)] = method\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame, y=None, name=None) -> pd.DataFrame:\n",
    "        \"\"\"Применяет исправления\"\"\"\n",
    "        df = X.copy()\n",
    "        \n",
    "        for col in self.columns:\n",
    "            if col not in df.columns:\n",
    "                continue\n",
    "\n",
    "            if self.strategy == 'auto':\n",
    "                for invalid_val, method in self.values_dict.items():\n",
    "                    mask = df[col] == invalid_val\n",
    "                    if mask.any():\n",
    "                        fill_val = self.fill_values[(col, invalid_val)]\n",
    "                        \n",
    "                        # Определяем тип замены для вывода\n",
    "                        if method in ['median', 'mean', 'mode']:\n",
    "                            print(f\"- Заменено {mask.sum()} значений '{invalid_val}' в '{col}' на {fill_val} ({method})\")\n",
    "                        else:\n",
    "                            print(f\"- Заменено {mask.sum()} значений '{invalid_val}' в '{col}' на '{fill_val}'\")\n",
    "                        \n",
    "                        df.loc[mask, col] = fill_val\n",
    "                        \n",
    "            elif self.strategy == 'func' and self.func:\n",
    "                df[col] = df[col].apply(self.func)\n",
    "                \n",
    "            elif self.strategy == 'dict' and self.values_dict:\n",
    "                df[col] = df[col].replace(self.values_dict)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        return self.fit(X, y).transform(X, **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d3e518",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecimalPointChanger(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Класс для замены разделителя дроби в строковых столбцах для обеих выборок\"\"\"\n",
    "\n",
    "    def __init__(self, columns: List[str], skip_on_test=False):\n",
    "        \"\"\"Инициализация заменщика дроби в строковых столбцах\"\"\"\n",
    "        \n",
    "        self.columns = columns  # список столбцов, в которых нужно заменить разделитель дроби\n",
    "        self.skip_on_test = skip_on_test\n",
    "    \n",
    "    def fit_transform(self, X: Union[pd.DataFrame, np.array], y: None = None, **fit_params) -> np.ndarray:\n",
    "        \n",
    "        \"\"\"Непосредственно заменяет разделитель дроби запятую на точку\"\"\"\n",
    "\n",
    "        print('- Определяю необходимость замены запятой на точку')\n",
    "        \n",
    "        df = X.copy()\n",
    "\n",
    "        # Если columns не указаны, обрабатываем все столбцы\n",
    "        cols_to_process = self.columns if self.columns else df.columns\n",
    "\n",
    "        for col in cols_to_process:\n",
    "            if col in df.columns:\n",
    "                # Проверяем, есть ли запятые в столбце\n",
    "                if df[col].dtype == 'object' and df[col].str.contains(',').any():\n",
    "                    df[col] = df[col].str.replace(',', '.').astype(float)\n",
    "                    print(f'- Заменил запятую на точку в столбце {col}')\n",
    "                    print(f'--- Значения в столбце {col}: {df[col].unique()}\\n')\n",
    "                else:\n",
    "                    print(f'- В столбце {col} замена не требуется')\n",
    "        \n",
    "        print('- Обработка завершена\\n')\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c2b940",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutlierHandler(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Универсальный класс для обработки выбросов\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        skip_on_test=True,\n",
    "        target_columns: Optional[List[str]] = None, \n",
    "        columns: Optional[List[str]] = None,\n",
    "        method: str = 'IQR',\n",
    "        action: str = 'winsorize',\n",
    "        factor: float = 1.5,\n",
    "        clip_quantiles: tuple = (0.01, 0.99),\n",
    "        IQR_quantiles: tuple = (0.25, 0.75),\n",
    "        extreme_factor: float = 3.0,\n",
    "        min_valid_values: Optional[Dict[str, float]] = None,\n",
    "        max_valid_values: Optional[Dict[str, float]] = None\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        columns: list of str optional, default=None (обрабатываются все числовые)\n",
    "            Столбцы для обработки. Если None, обрабатываются все числовые столбцы.\n",
    "\n",
    "        skip_on_test: bool, default=True\n",
    "            Если True, то для тестовой выборки обрабатываются только target_columns (если они указаны), остальные колонки пропускаются. Если False, то обрабатываются все колонки\n",
    "        \n",
    "        target_columns: list of str optional, default=None \n",
    "            Список целевых колонок для тестовой выборки, которые нужно обработать. Если None, то на тесте ничего не обрабатывается.\n",
    "\n",
    "        method: str, default='IQR'\n",
    "            'IQR' или 'quantile'\n",
    "\n",
    "        action: str, default='winsorize'\n",
    "            'remove' (всю строку), 'nan', 'mean', 'clip' (замещение выбросов граничным значением), 'winsorize' (умная обработка)\n",
    "\n",
    "        factor: float, default=1.5\n",
    "            Множитель для IQR (только для IQR метода)    \n",
    "\n",
    "        extreme_factor: float, default=3.0\n",
    "            Множитель для экстремальных выбросов (только для winsorize)\n",
    "\n",
    "        IQR_quantiles: tuple, default=(0.25, 0.75)\n",
    "            Квантили для IQR метода\n",
    "\n",
    "        clip_quantiles: tuple, default=(0.01, 0.99)\n",
    "            Квантили для clip метода\n",
    "\n",
    "        min_valid_values: dict, default=None\n",
    "            Словарь с минимально допустимыми значениями для каждой колонки \n",
    "\n",
    "        max_valid_values: dict, default=None\n",
    "            Словарь с максимально допустимыми значениями для каждой колонки\n",
    "        \"\"\"\n",
    "\n",
    "        self.skip_on_test = skip_on_test\n",
    "        self.target_columns = target_columns or []\n",
    "        self.target_handling = bool(self.target_columns)  # флаг, указывающий, нужно ли обрабатывать только целевые колонки в тесте\n",
    "        self.columns = columns\n",
    "        self.method = method\n",
    "        self.action = action\n",
    "        self.factor = factor\n",
    "        self.extreme_factor = extreme_factor\n",
    "        self.clip_quantiles = clip_quantiles\n",
    "        self.IQR_quantiles = IQR_quantiles\n",
    "        self.bounds_dict = {}                   # словарь с границами, который был создан в методе fit() выбросов может и не быть\n",
    "        self.means_dict = {}\n",
    "        self.min_valid_values = min_valid_values\n",
    "        self.max_valid_values = max_valid_values\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None, **kwargs):\n",
    "        \"\"\"Запоминает границы. Только для train выборки\"\"\"\n",
    "\n",
    "        X_clean = X.copy()\n",
    "    \n",
    "        # Удаляем физически невозможные значения ПЕРЕД расчетом IQR\n",
    "        if self.min_valid_values:\n",
    "            for col, min_val in self.min_valid_values.items():\n",
    "                if col in X_clean.columns:\n",
    "                    X_clean = X_clean[X_clean[col] >= min_val]\n",
    "        \n",
    "        if self.max_valid_values:\n",
    "            for col, max_val in self.max_valid_values.items():\n",
    "                if col in X_clean.columns:\n",
    "                    X_clean = X_clean[X_clean[col] <= max_val]\n",
    "\n",
    "        cols = self.columns if self.columns else X.select_dtypes(include=[np.number]).columns\n",
    "        \n",
    "        if self.method == 'quantile':\n",
    "            for col in cols:\n",
    "                lower = X[col].quantile(self.clip_quantiles[0])\n",
    "                upper = X[col].quantile(self.clip_quantiles[1])\n",
    "                self.bounds_dict[col] = (lower, upper)\n",
    "        else:  # IQR\n",
    "            for col in cols:\n",
    "                Q1 = X[col].quantile(self.IQR_quantiles[0])\n",
    "                Q3 = X[col].quantile(self.IQR_quantiles[1])\n",
    "                IQR = Q3 - Q1\n",
    "                lower = Q1 - self.factor * IQR\n",
    "                upper = Q3 + self.factor * IQR\n",
    "                self.bounds_dict[col] = (lower, upper)\n",
    "        \n",
    "        if self.action == 'mean':\n",
    "            for col in cols:\n",
    "                self.means_dict[col] = X[col].mean()\n",
    "        \n",
    "        return self\n",
    "\n",
    "\n",
    "    def transform(self, X, name=None):\n",
    "        \"\"\"Применяет обработку. Для test пропускает все, кроме target_columns\"\"\"\n",
    "\n",
    "        X_transformed = X.copy()\n",
    "        cols = self.columns if self.columns else X.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "        # Если test - обрабатываем только target_columns\n",
    "        if self.skip_on_test and name and ('test' in name.lower() or 'val' in name.lower()) and self.target_handling:\n",
    "            cols = [col for col in cols if col in self.target_columns]\n",
    "            if not cols:\n",
    "                print(f\"- Пропускаю обработку выбросов для {name}\")\n",
    "                return X\n",
    "            print(f\"- Test выборка: обрабатываю только целевые колонки: {cols}\")\n",
    "\n",
    "        # БЛОК 1: Удаление физически невозможных значений (СНАЧАЛА)\n",
    "        # Удаление физически невозможных значений (например, возраст < 0 или пробег > 1000000, пробег < 0, цена < 0, год  выпуск авто 1500 и т.д.)\n",
    "        if self.min_valid_values:\n",
    "            for col, min_val in self.min_valid_values.items():\n",
    "                if col in cols and col in X_transformed.columns:\n",
    "                    invalid_mask = X_transformed[col] < min_val\n",
    "                    \n",
    "                    if invalid_mask.any():\n",
    "                        invalid_values = X_transformed.loc[invalid_mask, col]\n",
    "                        count = invalid_mask.sum()\n",
    "                        min_invalid = invalid_values.min()\n",
    "                        max_invalid = invalid_values.max()\n",
    "                        \n",
    "                        print(f\"\\n⚠️ Обнаружены недопустимые значения в '{col}':\")\n",
    "                        print(f\"   Количество: {count} строк\")\n",
    "                        print(f\"   Недопустимый диапазон: [{min_invalid:.2f} - {max_invalid:.2f}]\")\n",
    "                        print(f\"   Минимально допустимое значение: {min_val}\")\n",
    "                        print(f\"   → Строки будут удалены как явный шум (независимо от IQR)\")\n",
    "                        \n",
    "                        X_transformed = X_transformed[~invalid_mask]\n",
    "\n",
    "        if self.max_valid_values:\n",
    "            for col, max_val in self.max_valid_values.items():\n",
    "                if col in cols and col in X_transformed.columns:\n",
    "                    invalid_mask = X_transformed[col] > max_val\n",
    "                    \n",
    "                    if invalid_mask.any():\n",
    "                        invalid_values = X_transformed.loc[invalid_mask, col]\n",
    "                        count = invalid_mask.sum()\n",
    "                        min_invalid = invalid_values.min()\n",
    "                        max_invalid = invalid_values.max()\n",
    "                        \n",
    "                        print(f\"\\n⚠️ Обнаружены недопустимые значения в '{col}':\")\n",
    "                        print(f\"   Количество: {count} строк\")\n",
    "                        print(f\"   Недопустимый диапазон: [{min_invalid:.2f} - {max_invalid:.2f}]\")\n",
    "                        print(f\"   Максимально допустимое значение: {max_val}\")\n",
    "                        print(f\"   → Строки будут удалены как явный шум (независимо от IQR)\")\n",
    "                        \n",
    "                        X_transformed = X_transformed[~invalid_mask]\n",
    "\n",
    "\n",
    "        # Этот код не ищет выбросы — он просто проверяет, есть ли выбросы в каждой колонке.\n",
    "        # Чтобы Вывести предупреждение ниже, в каких колонках найдены выбросы\n",
    "        outlier_cols = []\n",
    "        for col in cols:\n",
    "            if col in self.bounds_dict:\n",
    "                lower, upper = self.bounds_dict[col]\n",
    "                outliers = (X_transformed[col] < lower) | (X_transformed[col] > upper)\n",
    "                if outliers.any():\n",
    "                    outlier_cols.append(col)\n",
    "        \n",
    "        if not outlier_cols:\n",
    "            print('- Выбросы не обнаружены')\n",
    "            return X_transformed\n",
    "        \n",
    "        print(f'\\n- Обнаружены выбросы в столбцах: {outlier_cols}')\n",
    "        print(f'- Метод: {self.method}, Действие: {self.action}\\n')\n",
    "\n",
    "\n",
    "        # ВЫВОД ГРАНИЦ ДЛЯ КАЖДОЙ КОЛОНКИ\n",
    "        for col in outlier_cols:\n",
    "            if col in self.bounds_dict:\n",
    "                lower, upper = self.bounds_dict[col]\n",
    "                print(f'  Нормальные пределы для {col}: [{lower:.2f} - {upper:.2f}]')\n",
    "        print()\n",
    "        \n",
    "\n",
    "        # ВИНЗОРИЗАЦИЯ (умная обработка)\n",
    "        if self.action == 'winsorize':\n",
    "            print('- Применяю винзоризацию (мягкие → clip, экстремальные → remove)')\n",
    "            X_transformed['outlier_status'] = 'normal'\n",
    "            \n",
    "            for col, (lower, upper) in self.bounds_dict.items():\n",
    "                # Границы для экстремальных выбросов\n",
    "                IQR = upper - lower\n",
    "                extreme_lower = lower - self.extreme_factor * IQR\n",
    "                extreme_upper = upper + self.extreme_factor * IQR\n",
    "                \n",
    "                # Мягкие выбросы → винзоризация (clipping)\n",
    "                mild_outliers = ((X_transformed[col] < lower) & (X_transformed[col] >= extreme_lower)) | \\\n",
    "                               ((X_transformed[col] > upper) & (X_transformed[col] <= extreme_upper))\n",
    "                X_transformed.loc[mild_outliers, 'outlier_status'] = 'mild'\n",
    "                X_transformed.loc[mild_outliers, col] = X_transformed.loc[mild_outliers, col].clip(lower, upper)\n",
    "                \n",
    "                # Экстремальные выбросы → маркировка для удаления\n",
    "                extreme_outliers = (X_transformed[col] < extreme_lower) | (X_transformed[col] > extreme_upper)\n",
    "                X_transformed.loc[extreme_outliers, 'outlier_status'] = 'extreme'\n",
    "            \n",
    "            # Удаляем только экстремальные\n",
    "            n_extreme = (X_transformed['outlier_status'] == 'extreme').sum()\n",
    "            if n_extreme > 0:\n",
    "                print(f'Удалено экстремальных выбросов: {n_extreme}')\n",
    "                X_transformed = X_transformed[X_transformed['outlier_status'] != 'extreme']\n",
    "            \n",
    "            X_transformed = X_transformed.drop('outlier_status', axis=1)\n",
    "        \n",
    "\n",
    "        # Остальные действия (clip, nan, mean, remove)\n",
    "        elif self.action == 'clip':\n",
    "            for col, (lower, upper) in self.bounds_dict.items():\n",
    "                X_transformed[col] = X_transformed[col].clip(lower, upper)\n",
    "        \n",
    "        elif self.action == 'nan':\n",
    "            for col, (lower, upper) in self.bounds_dict.items():\n",
    "                outliers = (X_transformed[col] < lower) | (X_transformed[col] > upper)\n",
    "                X_transformed.loc[outliers, col] = np.nan\n",
    "        \n",
    "        elif self.action == 'mean':\n",
    "            for col, (lower, upper) in self.bounds_dict.items():\n",
    "                outliers = (X_transformed[col] < lower) | (X_transformed[col] > upper)\n",
    "                X_transformed.loc[outliers, col] = self.means_dict[col]\n",
    "        \n",
    "        elif self.action == 'remove':\n",
    "            if self.columns is None:\n",
    "                mask = pd.Series([True] * len(X_transformed), index=X_transformed.index)\n",
    "                for col, (lower, upper) in self.bounds_dict.items():\n",
    "                    outliers = (X_transformed[col] < lower) | (X_transformed[col] > upper)\n",
    "                    mask &= ~outliers\n",
    "                X_transformed = X_transformed[mask]\n",
    "                X_transformed = X_transformed.reset_index(drop=True)\n",
    "            else:\n",
    "                for col, (lower, upper) in self.bounds_dict.items():\n",
    "                    outliers = (X_transformed[col] < lower) | (X_transformed[col] > upper)\n",
    "                    X_transformed = X_transformed[~outliers]\n",
    "                    X_transformed = X_transformed.reset_index(drop=True)\n",
    "        \n",
    "        return X_transformed\n",
    "\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):   # name=None передается через **fit_params, чтобы не ломать сигнатуру метода\n",
    "        return self.fit(X, y).transform(X, **fit_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70eaefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImplicitDuplicatesViewer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Выводит уникальные значения каждого столбца для визуального определения неявных дубликатов только для тренировочной выборки\"\"\"\n",
    "\n",
    "    def __init__(self, skip_on_test=True, columns: List[str] = None):\n",
    "        \"\"\"Инициализация определеителя неявных дуликатов\"\"\"\n",
    "        self.columns = columns\n",
    "        self.skip_on_test = skip_on_test\n",
    "\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: None = None):\n",
    "        return self\n",
    "\n",
    "\n",
    "    def transform(self, X: pd.DataFrame, y: None = None, name=None):\n",
    "        \"\"\"Выводит уникальные значения каждого столбца для визуального определения неявных дуликатов\"\"\"\n",
    "\n",
    "        if self.skip_on_test and name and ('test' in name.lower() or 'val' in name.lower()):\n",
    "            print(f\"- Пропускаю проверку на неявные дубликаты для {name}\")\n",
    "            return X\n",
    "\n",
    "\n",
    "        print('- Выполняю поиск неявных дубликатов в нечисловых столбцах')\n",
    "\n",
    "        df = X.copy()\n",
    "\n",
    "        if self.columns:\n",
    "            for col in self.columns:\n",
    "                if col in df.columns:\n",
    "                    print(f'- Уникальные значения в столбце {col}: {sorted(X[col].unique().tolist())}\\n')\n",
    "            return X\n",
    "        else: \n",
    "            # columns = X.select_dtypes(exclude=[np.number]).columns # проверить только нечисловвые ячейки\n",
    "            for col in df.columns:\n",
    "                print(f'- Уникальные значения в столбце {col}: {sorted(X[col].unique().tolist())}\\n')\n",
    "            return X\n",
    "        \n",
    "    def fit_transform(self, X, y: None = None, **fit_params):   # name=None передается через **fit_params, чтобы не ломать сигнатуру метода\n",
    "        return self.fit(X, y).transform(X, **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fecedc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuplicateRemover(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Класс для удаления дубликатов только для тренировочной выборки\"\"\"\n",
    "\n",
    "    def __init__(self, skip_on_test=True, columns: List[str] = None):\n",
    "        \"\"\"Инициализация удалителя дубликатов\"\"\"\n",
    "        self.columns = columns\n",
    "        self.skip_on_test = skip_on_test\n",
    "\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: None = None):\n",
    "        return self\n",
    "\n",
    "\n",
    "    def transform(self, X: pd.DataFrame, y: None = None, name=None):\n",
    "        \"\"\"Удаляет дубликаты только в тренировочной выборке\"\"\"\n",
    "\n",
    "        if self.skip_on_test and name and ('test' in name.lower() or 'val' in name.lower()):\n",
    "            print(f\"- Пропускаю удаление дубликатов для {name}\")\n",
    "            return X\n",
    "        \n",
    "        duplicate_count = X.duplicated().sum()\n",
    "\n",
    "        if duplicate_count:\n",
    "            print(f'- Выявлено {duplicate_count} дубликатов')\n",
    "            print('- Выполняю удаление дубликатов\\n')\n",
    "\n",
    "            if self.columns:\n",
    "                X = X.drop_duplicates(subset=self.columns)                \n",
    "            else:\n",
    "                X = X.drop_duplicates()\n",
    "            \n",
    "            remaining_duplicates = X.duplicated().sum()\n",
    "            print(f'- Осталось {remaining_duplicates} дубликатов\\n')\n",
    "        else:\n",
    "            print('- Дубликатов не выявлено\\n')\n",
    "        \n",
    "        return X  # ← теперь возвращаешь очищенный X\n",
    "\n",
    "    \n",
    "    def fit_transform(self, X, y: None = None, **fit_params):   # name=None передается через **fit_params, чтобы не ломать сигнатуру метода\n",
    "        return self.fit(X, y).transform(X, **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5425b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MissingValueHandler(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Класс для обработки пропущенных значений в данных. Возможные варианты параметра strategy: mean, median, mode, drop, unknown. По умолчанию drop. Для обеих выборок.\"\"\"\n",
    "\n",
    "    def __init__(self, skip_on_test=False, strategy='mean', fill_value=None):\n",
    "        \"\"\"Инициализация обработчика пропущенных значений. По умолчанию заполняет средним значением.\n",
    "        \"\"\"\n",
    "        self.skip_on_test = skip_on_test\n",
    "        self.strategy = strategy\n",
    "        self.fill_value = fill_value\n",
    "        self.fill_values_ = {}  # для хранения значений для заполнения\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: None=None):\n",
    "        \"\"\"Запоминает значения из train\"\"\"\n",
    "        if self.strategy == 'mean':\n",
    "            self.fill_values_ = X.select_dtypes(include=[np.number]).mean().to_dict()\n",
    "        elif self.strategy == 'median':\n",
    "            self.fill_values_ = X.select_dtypes(include=[np.number]).median().to_dict()\n",
    "        elif self.strategy == 'mode':\n",
    "            self.fill_values_ = {col: X[col].mode()[0] for col in X.columns}\n",
    "        elif self.strategy == 'unknown':\n",
    "            self.fill_values_ = {}  # для unknown не нужно запоминать\n",
    "        elif self.strategy == 'drop':\n",
    "            self.fill_values_ = {}  # для drop не нужно запоминать\n",
    "    \n",
    "        return self\n",
    "        \n",
    "    def transform(self, X: pd.DataFrame, y: None = None, name=None, **fit_params):\n",
    "        \"\"\"Применяет значения, которые запомнил из train. Заполняет пропущенные значения или удаляет строки в тестовой выборке. режимы: mean, median, mode, drop\"\"\"\n",
    "        \n",
    "        df = X.copy()\n",
    "\n",
    "        if self.strategy == 'unknown':\n",
    "            for col in df.select_dtypes(include='object').columns:\n",
    "                if df[col].isnull().any():\n",
    "                    df[col] = df[col].fillna('unknown')\n",
    "\n",
    "        # Стратегия drop — удаление строк\n",
    "        if self.strategy == 'drop':\n",
    "            null_count = df.isna().sum().sum()\n",
    "            if null_count > 0 or df.eq(\" \").any().any():\n",
    "                print('- Нашел пропуски в данных\\n')\n",
    "                null_string_count = len(df[df.isna().any(axis=1)])\n",
    "                display(df[df.isna().any(axis=1)])\n",
    "                if len(df[df.eq(\" \").any(axis=1)]) != 0:\n",
    "                    display(df[df.eq(\" \").any(axis=1)])\n",
    "                \n",
    "                null_string_percentage = null_string_count / len(df) * 100\n",
    "                if null_string_percentage < 10:\n",
    "                    print(f'- Выявлено {null_count} пропусков в {null_string_count} строках ({null_string_percentage:.2f}%). Удаляю\\n')\n",
    "                    df = df.dropna()\n",
    "                    df = df[~df.eq(\" \").any(axis=1)]\n",
    "                    print(f'- Осталось {df.isna().sum().sum()} пропусков\\n')\n",
    "            else:\n",
    "                print('- Пропусков не найдено\\n')\n",
    "            return df\n",
    "        \n",
    "        # Остальные стратегии — заполнение запомненными значениями\n",
    "        for col, fill_value in self.fill_values_.items():\n",
    "            if col in df.columns and df[col].isnull().any():\n",
    "                count = df[col].isnull().sum()\n",
    "                percent = (count / len(df)) * 100\n",
    "                print(f\"- '{col}': {count} пропусков ({percent:.1f}%). Заполняю '{self.strategy}' → {fill_value}\")\n",
    "                df[col] = df[col].fillna(fill_value)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, X: pd.DataFrame, y: None = None, **fit_params):  # name=None передается через **fit_params, чтобы не ломать сигнатуру метода\n",
    "        return self.fit(X, y).transform(X, **fit_params)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2119591f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnRemover(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Удаляет лишние колонки, переданные в списке. Работает как для train, так и для test выборки\"\"\" \n",
    "\n",
    "    def __init__(self, columns: List[str], skip_on_test=False):\n",
    "        self.columns = columns\n",
    "        self.skip_on_test=skip_on_test\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: None=None, name=None):\n",
    "        # Просто сохраняем информацию о столбцах для удаления\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame, name=None):\n",
    "\n",
    "        df = X.copy()\n",
    "\n",
    "        for col in self.columns:\n",
    "            if col in df.columns:\n",
    "                df = df.drop(col, axis=1)\n",
    "                print(f'- Удалил столбец {col}')\n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, X: pd.DataFrame, y: None=None, **fit_params):  # name=None передается через **fit_params, чтобы не ломать сигнатуру метода\n",
    "        return self.fit(X, y).transform(X, **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093d39ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FloatToIntChanger(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Преобразует дробные значения в целочисленные (режим Multiplie - по умолчанию: \n",
    "    умножает на 100 и сохраняет как Int, simple: без умножения меняет тип) на основе переданного списка столбцов\n",
    "    \"\"\"    \n",
    "\n",
    "    def __init__(self, columns, strategy, skip_on_test=False):\n",
    "        self.columns = columns\n",
    "        self.strategy = strategy\n",
    "        self.skip_on_test = False\n",
    "\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y=None):\n",
    "        # Вызывается только для train. Просто сохраняем информацию о столбцах формально\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            self.feature_names_in_ = X.columns.tolist()\n",
    "        else:\n",
    "            # Если на входе массив, генерируем имена колонок\n",
    "            self.feature_names_in_ = [f\"col_{i}\" for i in range(X.shape[1])]\n",
    "        return self\n",
    "    \n",
    "\n",
    "    def transform(self, X: pd.DataFrame, y=None, name=None):\n",
    "\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            df = X.copy()\n",
    "        else:\n",
    "            # Восстанавливаем DataFrame из массива\n",
    "            df = pd.DataFrame(X, columns=self.feature_names_in_)\n",
    "\n",
    "        if self.strategy == 'simple':\n",
    "            for col in self.columns:\n",
    "                if col in df.columns:\n",
    "                    print(f'\\n - Меняю тип на int столбце {col}')\n",
    "                    df = df[col].astype('int')\n",
    "            return df\n",
    "\n",
    "        if self.strategy == 'multiplie':\n",
    "            for col in self.columns:\n",
    "                if col in df.columns:\n",
    "                    print(f'\\n - Значения в столбце {col} умножаю на 100 ')\n",
    "                    df[col] = (df[col] * 100).astype('int')\n",
    "                else:\n",
    "                    print(f'- Колонка {col} не найдена в данных')\n",
    "            return df\n",
    "\n",
    "    def fit_transform(self, X: pd.DataFrame, y=None, **fit_params):\n",
    "        return self.fit(X, y).transform(X, **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cff468",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EDAPreprocessor:\n",
    "    \"\"\"\n",
    "    Основной класс пайплайна для предобработки данных\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, func: Callable[..., Any] | None = None):\n",
    "        self.steps = []  # список шагов предобработки, которые будут выполняться в пайплайне\n",
    "        self.fitted_transformers = {}  # для хранения обученных трансформеров (запоминает что-то и делает)\n",
    "        \n",
    "    def add_mistake_corrector(\n",
    "                                self, \n",
    "                                columns: List[str] | None = None,\n",
    "                                values_dict: dict | None = None, \n",
    "                                func: Callable[..., Any] | None = None, \n",
    "                                strategy: str | None = None, \n",
    "                                skip_on_test: bool = False,\n",
    "                                step_name: str =' Преобразование некорректных данных'):\n",
    "        \"\"\"Добавляет шаг исправления ошибок в препроцессор, принимает на вход список колонок, в которых произвести замены, словарь с неверными и верными значениями\"\"\"\n",
    "        \n",
    "        mistake_corrector = MistakeCorrector(\n",
    "                                                columns=columns, \n",
    "                                                values_dict=values_dict, \n",
    "                                                func=func,\n",
    "                                                strategy=strategy\n",
    "            )\n",
    "        self.steps.append((step_name, mistake_corrector))\n",
    "        return self\n",
    "\n",
    "    def add_column_remover(\n",
    "            self, \n",
    "            columns: List[str], \n",
    "            skip_on_test: bool = False,\n",
    "            step_name: str = 'Удаление столбцов'):\n",
    "        \n",
    "        column_remover=ColumnRemover(columns=columns)\n",
    "        self.steps.append((step_name, column_remover))\n",
    "        return self\n",
    "\n",
    "    def add_float_to_int_changer(\n",
    "            self, \n",
    "            columns:List[str] | None = None,\n",
    "            strategy='multiplie', \n",
    "            skip_on_test: bool = False,\n",
    "            step_name='Преобразование дробных чисел в целочисленное'):\n",
    "        float_to_int_changer=FloatToIntChanger(columns, strategy)\n",
    "        self.steps.append((step_name, float_to_int_changer))\n",
    "        return self\n",
    "\n",
    "    def add_decimal_point_changer(\n",
    "            self, \n",
    "            columns:List[str] | None = None, \n",
    "            skip_on_test: bool = False,\n",
    "            step_name='Замена запятой на точку в дробных числах при необходимости'):\n",
    "        decimal_point_changer = DecimalPointChanger(columns)\n",
    "        self.steps.append((step_name, decimal_point_changer))\n",
    "        return self\n",
    "\n",
    "\n",
    "    def add_missing_value_handler(\n",
    "            self, \n",
    "            strategy='drop', \n",
    "            fill_value=None, \n",
    "            skip_on_test: bool = False,\n",
    "            step_name='Проверка пропущенных значений'):\n",
    "        \"\"\"Добавляет обработчик пропущенных значений в препроцессор\"\"\"\n",
    "        missing_handler = MissingValueHandler(strategy=strategy, fill_value=fill_value)\n",
    "        self.steps.append((step_name, missing_handler))\n",
    "        return self    \n",
    "\n",
    "\n",
    "    def add_outlier_handler(\n",
    "            self, \n",
    "            columns: Optional[List[str]] = None,\n",
    "            target_columns:Optional[List[str]] = None,\n",
    "            method: str = 'IQR',\n",
    "            action: str = 'winsorize',\n",
    "            factor: float = 1.5,\n",
    "            extreme_factor: float = 3.0,\n",
    "            min_valid_values: Optional[Dict[str, float]] = None,\n",
    "            max_valid_values: Optional[Dict[str, float]] = None,\n",
    "            skip_on_test: bool = True,\n",
    "            step_name: str = 'Проверка на наличие выбросов'):\n",
    "        '''Добавляет шаг обработки выбросов'''\n",
    "        outlier_handler = OutlierHandler(columns=columns, method=method, action=action, factor=factor, extreme_factor=extreme_factor, min_valid_values=min_valid_values, max_valid_values=max_valid_values, skip_on_test=skip_on_test)\n",
    "        self.steps.append((step_name, outlier_handler))\n",
    "        return self\n",
    "\n",
    " \n",
    "    \n",
    "    def add_drop_duplicates(\n",
    "            self, \n",
    "            skip_on_test: bool = True,\n",
    "            step_name='Проверка на наличие явных дубликатов'):\n",
    "        \"\"\"Добавляет шаг удаления дуликатов\"\"\"\n",
    "        duplicate_remover = DuplicateRemover(skip_on_test=skip_on_test)\n",
    "        self.steps.append((step_name, duplicate_remover))\n",
    "        return self\n",
    "    \n",
    "\n",
    "    def add_implicit_duplicates_viewer(\n",
    "            self, \n",
    "            skip_on_test: bool = True,\n",
    "            columns:List[str] | None = None, \n",
    "            step_name='Отображение неявных дубликатов и проверка на неоднородность данных'):\n",
    "        \"\"\"Добавляет шаг отображения уникальных значений каждого столбца для выявления неявных дуликатов визуально.\"\"\"\n",
    "        implict_duplicates_viewer = ImplicitDuplicatesViewer(columns=columns, skip_on_test=skip_on_test)\n",
    "        self.steps.append((step_name, implict_duplicates_viewer))\n",
    "        return self\n",
    "    \n",
    "\n",
    "    def add_custom_transformer(self, step_name: str, transformer):\n",
    "        \"\"\"Добавляет пользовательский трансформер в пайплан предобраотки\"\"\"\n",
    "        self.steps.append((step_name, transformer))\n",
    "        return self\n",
    "    \n",
    "\n",
    "    def fit_transform(self, X: pd.DataFrame, name, y: None=None):\n",
    "        \"\"\"Обучает все трансформеры пайплайна (запоминает единые параметры и условия исполнения - консистентность). Принимает df и целевую переменную опционально.\"\"\"\n",
    "\n",
    "        df = X.copy()\n",
    "\n",
    "        for i, (step_name, transformer) in enumerate(self.steps):\n",
    "            print(f'\\nИсполнение шага {i+1}: {step_name}')\n",
    "            df = transformer.fit_transform(df, name=name)\n",
    "            self.fitted_transformers[step_name] = transformer\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f201c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA_Preprocessor_pipeline = (\n",
    "    EDAPreprocessor()  \n",
    "    .add_mistake_corrector(columns=['registration_month'], values_dict={0: 'median'}, strategy='auto', skip_on_test=False)\n",
    "    .add_mistake_corrector(columns=['fuel_type'], values_dict={'gasoline': 'petrol'}, strategy='auto', skip_on_test=False)\n",
    "    .add_column_remover(columns=['date_crawled', 'date_created', 'last_seen', 'number_of_pictures'], skip_on_test=False)\n",
    "    .add_decimal_point_changer()                            \n",
    "    .add_outlier_handler(skip_on_test=True, target_columns=['price'], method='IQR', action='winsorize', factor=1.5, extreme_factor=3.0, min_valid_values={'price': 1, 'power': 1, 'registration_year': 1920}, max_valid_values={'power': 1900, 'registration_year': 2016}) \n",
    "                                           # IQR, clip   # remove, nan, mean, clip, winsorize                                  \n",
    "    .add_missing_value_handler(skip_on_test=False, strategy='unknown')   # на тестовой только заполнять, никогда не удалять         \n",
    "                                                   # mean, median, mode, unknown, drop\n",
    "    .add_drop_duplicates(skip_on_test=True)                                    \n",
    "    .add_implicit_duplicates_viewer(columns=None, skip_on_test=True)           \n",
    "\n",
    ")\n",
    "\n",
    "print('Вот таким у нас получился предобработчик данных.\\n')\n",
    "\n",
    "print(\"Шаги в пайплайне:\\n\")\n",
    "for i, (name, step) in enumerate(EDA_Preprocessor_pipeline.steps):\n",
    "    print(f\"{i+1}. {name}: {step}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfbfcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_preprocessor(df_dict=df_dict):\n",
    "    \"\"\"Производит предобработку всех датафреймов в цикле в пайплайне\"\"\"\n",
    "    for name, df in df_dict.items():\n",
    "        print('=' * 50)\n",
    "        print(f' =>  Обработка датафрейма {name}')\n",
    "        print('=' * 50)\n",
    "        df_dict[name] = EDA_Preprocessor_pipeline.fit_transform(df, name)\n",
    "        globals()[name] = df_dict[name]  # Перезаписывает глобальную переменную\n",
    "        print(f'\\nПроверка датафрейма {name}')\n",
    "        display(df_dict[name].head())\n",
    "        display(df_dict[name].info())\n",
    "        print(f'Обработка датафрейма {name} завершена.\\n\\n')\n",
    "\n",
    "    # Объединить три выборки\n",
    "    df = pd.concat([train_df, test_df, val_df], ignore_index=True)\n",
    "\n",
    "    print('Выведем описательную статистику по всем трем датиафреймам после обработки в сумме, чтобы посмотреть, что осталось в итоге.')\n",
    "    print('ОЦЕНКА СТАТИСТИЧЕСКИ ОПИСАТЕЛЬНОЙ СТАТИСТИКИ ПО ВСЕМ ДАТАФРЕЙМАМ В СУММЕ ПОСЛЕ ПРЕДОБРАБОТКИ\\n')\n",
    "\n",
    "    # Общий describe\n",
    "    display(df.describe())    \n",
    "\n",
    "run_preprocessor(df_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83339b07",
   "metadata": {},
   "source": [
    "### **3.5. Классификация признаков по типам**\n",
    "\n",
    "Перед построением распределений признаков и обучений модели необходимо классифицировать их по способу обработки, а не по математической классификации: \n",
    "\n",
    "- **числовые**, \n",
    "- **категориальные**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601e8e38",
   "metadata": {},
   "source": [
    "**1. Числовые** — используются как есть:\n",
    "\n",
    "- `RegistrationYear` — непрерывный\n",
    "\n",
    "- `Power` — непрерывный\n",
    "\n",
    "- `Kilometer` — дискретный (но обрабатывается как непрерывный)\n",
    "\n",
    "- `RegistrationMonth` — дискретный\n",
    "\n",
    "- `PostalCode` — числовой (но может быть категориальным, регион влияет на цену, но 99к уникальных значений, можно агрегировать)\n",
    "\n",
    "**2. Категориальные** — требуют кодирования:\n",
    "\n",
    "- `VehicleType` — номинальный (sedan, suv, coupe...)\n",
    "\n",
    "- `Gearbox` — номинальный (manual, auto)\n",
    "\n",
    "- `Model` — номинальный (golf, passat...)\n",
    "\n",
    "- `FuelType` — номинальный (petrol, diesel...)\n",
    "\n",
    "- `Brand` — номинальный (volkswagen, audi...)\n",
    "\n",
    "- `Repaired` — бинарный (yes/no)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eba6b2",
   "metadata": {},
   "source": [
    "При визуализации признаков, их следует  делить на две большие группы `Дискретные` и `Непрерывные`.\n",
    "\n",
    "**1) Дискретные включают:**\n",
    "\n",
    "- бинарные;\n",
    "- категориальные \n",
    "- целые числа (месяц, год, количество, поддающееся несложному подсчету, когда можно перечислить все варианты)\n",
    "\n",
    "дискретные в нашем случае: `RegistrationYear`, `RegistrationMonth`, `Kilometer` (binned), `VehicleType`, `Gearbox`, `Brand`, `NotRepaired`. Обращаем внимание, что пробег авто прдствален в бинированном виде в признаке Kilometer и имеет всего пять значений [87500, 90000, 100000, 125000, 150000].\n",
    "\n",
    "Для визуализщации дискретных признаков рекомендуется использовать countplot из seaborn.\n",
    "\n",
    "**2) Непрерывные включают:**\n",
    "\n",
    "- дробные числа;\n",
    "- измерения;\n",
    "- время\n",
    "- когда бесконечно много значений в диапазоне.\n",
    "\n",
    "Непрерывные признаки в нашем случае: `Price`, `Power`, `PostalCode` (218k уникальных значений)\n",
    "\n",
    "Призанки делятся по-разному в зависмости от решаемой задачи на данный момент:\n",
    "\n",
    "- при визуализации признаки принято делить на длискретные и непрерывные.\n",
    "\n",
    "- при обучении модели делить на числовые и категориальные."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f648b8a2",
   "metadata": {},
   "source": [
    "## **3.6. Оценка распределения признаков**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67886c79",
   "metadata": {},
   "source": [
    "### **3.6.1. Распределение дискретных признаков**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d154b749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация дискретных признаков\n",
    "discrete_features = ['registration_year', 'registration_month', 'kilometer', \n",
    "                     'vehicle_type', 'gearbox', 'brand', 'repaired', 'fuel_type']\n",
    "\n",
    "fig, axes = plt.subplots(4, 2, figsize=(20, 16))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(discrete_features):\n",
    "    sns.countplot(data=train_df, x=feature, ax=axes[idx]) # , order=train_df[feature].value_counts().index\n",
    "    axes[idx].set_title(f'Распределение: {feature}')\n",
    "    axes[idx].tick_params(axis='x', rotation=70)\n",
    "    \n",
    "# axes[-1].axis('off')  # Скрыть последний пустой subplot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f188c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **3.6.1.1. Распределение категориальных признаков**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314836f1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Категориальные признаки входят в состав дискретных признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb85508",
   "metadata": {},
   "source": [
    "#### **3.6.1.2. Распределение дискретного высококардинального признака с 218k значений через countplot из seaborn.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610dd004",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['region'] = train_df['postal_code'] // 1000\n",
    "test_df['region'] = test_df['postal_code'] // 1000\n",
    "val_df['region'] = val_df['postal_code'] // 1000\n",
    "\n",
    "features = ['region', 'model']\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(25, 6))\n",
    "\n",
    "for idx, feature in enumerate(features):\n",
    "    sns.countplot(data=train_df, x=feature, ax=axes[idx], order=train_df[feature].value_counts().index)\n",
    "    axes[idx].set_title(f'Распределение по признаку {feature}')\n",
    "    axes[idx].set_xlabel(feature)\n",
    "    axes[idx].set_ylabel('Количество')\n",
    "    axes[idx].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2059b92f",
   "metadata": {},
   "source": [
    "#### **3.6.1.3. Облако слов для категориальных признаков**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a70a70",
   "metadata": {},
   "source": [
    "Построим облако слов для категориальных признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6ae194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объединяем строки в единый текст для каждого столбца\n",
    "brands_text = ' '.join(df['brand'].astype(str))\n",
    "models_text = ' '.join(df['model'].astype(str))\n",
    "\n",
    "# Создаем облако слов для брендов\n",
    "wc_brands = WordCloud(width=4000, height=3000).generate(brands_text)\n",
    "\n",
    "# Создаем облако слов для моделей автомобилей\n",
    "wc_models = WordCloud(width=4000, height=3000).generate(models_text)\n",
    "\n",
    "# Отображаем оба облака слов\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Облако слов брендов\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(wc_brands, interpolation='bilinear')\n",
    "plt.title('Облако брендов')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Облако слов моделей\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(wc_models, interpolation='bilinear')\n",
    "plt.title('Облако моделей')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d55ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### **3.6.2. Распределение непрерывных признаков**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a12f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "sns.histplot(data=train_df, x='price', bins=50, kde=True, ax=axes[0])\n",
    "axes[0].set_title('Распределение цены')\n",
    "\n",
    "sns.histplot(data=train_df, x='power', bins=50, kde=True, ax=axes[1])\n",
    "axes[1].set_title('Распределение мощности')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bf718d",
   "metadata": {},
   "source": [
    "## **3.7. Расшифровка визуализации распределения признаков**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635eb173",
   "metadata": {},
   "source": [
    "Несмотря на предобработку данных и удаление выбросов в частности, \n",
    "\n",
    "практически все признаки имеют скошенное распределение: Бренд, год регистрации, Тип топлива, тип авто, мощномть, пробег.\n",
    "\n",
    "Нормальное распределение у года регистрации. \n",
    "\n",
    "Вопросы остались по мощности авто что считать выбросами, будет ли данная модель предсказывать стоимость спорткаров или нет? Выбросы по мощности свыше 1500 л.с. мы удалили.\n",
    "\n",
    "К числовым признакам, имеющим скошенное распределение мы применим RobustScaler\n",
    "\n",
    "К категориальным признакам мы применим категорирование OneHotEncoder\n",
    "\n",
    "Месяц регистрации особой ценности признак не несет.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14446258",
   "metadata": {},
   "source": [
    "## **3.8. Оценка статистических предобработанных данных** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5633422",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27d3616",
   "metadata": {},
   "source": [
    "расшифровка дискрайба"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0367e4d5",
   "metadata": {},
   "source": [
    "**Рекомендации для обучения модели**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3ba4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872c9d8e",
   "metadata": {},
   "source": [
    "## **3.9. Корреляция данных**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a96a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_cols = interval_cols = ['price', 'power', 'kilometer', 'postal_code', 'age'] # только дробные числа, или которые не в силах посчитать руками (если очень много значений, значит интервальный тип)\n",
    "\n",
    "phik_corr = train_df.phik_matrix(interval_cols=interval_cols)\n",
    "phik_corr     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048e0da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "sns.heatmap(phik_corr.round(2), annot=True, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b83bb33",
   "metadata": {},
   "source": [
    "**Мультиколлинеарность наблюдается среди признаков:**\n",
    "\n",
    "- `RegistrationYear` и `Age` (возраст = 2016 - год) - почти полная корреляция\n",
    "- `Brand` и `Model` \n",
    "- `Kilometer` & `Kilometer_Max`\n",
    "\n",
    "**Высокая корреляция с целевым признаком у:**\n",
    "\n",
    "- `Age` - 0.67\n",
    "- `RegistrationYear` - 0.63  (но модель lgbm с ним обучается хуже)\n",
    "- `Model` - 0.58\n",
    "- `Power` - 0.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801476d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(train_df[interval_cols], plot_kws={'alpha': 0.3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dd71e0",
   "metadata": {},
   "source": [
    "## **3.10. Проверка данных на неоднородность: выявление нелинейных связей**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164971ae",
   "metadata": {},
   "source": [
    "Мы надеемся, что выявление скрытых связей поможет повысить качество обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fdee2b",
   "metadata": {},
   "source": [
    "**Мы применили:**\n",
    "\n",
    "1. **Тесты Левена и Бартлетта** для проверки однородности дисперсий; \n",
    "2. **Simpson's Paradox:** когда общая корреляция и внутри груцппы имеют противопаоложные знаки;\n",
    "3. Группировочный анализ;\n",
    "4. Стабильность корреляций;\n",
    "5. Скрытые кластеры;\n",
    "6. Нелинейные связи;\n",
    "7. Взаимодействия.\n",
    "\n",
    "**Выводы:**\n",
    "Скрытых взаимосвзей не обнаружено.  Казалось бы, что-то удалось найти, но при обучении модели они дают хуже результат, поэтому от этой информации пришлось отказаться при обучении модели. Блок удален за ненадобностью."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7877050",
   "metadata": {},
   "source": [
    "## **4. Feature Enginering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa749346",
   "metadata": {},
   "source": [
    "## **4.1. Новые признаки на основе анализа неоднородности**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb07c55",
   "metadata": {},
   "source": [
    "Создание кластера.\n",
    "\n",
    "Благоджаря кластеризации, модель понимает контекст - одинаковые характеристики могут означать разную цену в зависимости от сегмента.\n",
    "\n",
    "Кластеризация должна быть только на train, затем применяется к test через predict()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dcd750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для кластеризации используем ВСЕ важные признаки\n",
    "cluster_features = ['power', 'kilometer', 'age', 'price']  # можно добавить Region, но там много уникальных значений, может не сработать\n",
    "X_cluster = train_df[cluster_features].fillna(train_df[cluster_features].median())\n",
    "\n",
    "# Обучение\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "train_df['cluster'] = kmeans.fit_predict(X_cluster)\n",
    "X_test_cluster = test_df[cluster_features].fillna(train_df[cluster_features].median())\n",
    "test_df['cluster'] = kmeans.predict(X_test_cluster)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8e1398",
   "metadata": {},
   "source": [
    "Проведем ради интереса анлиз кластеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03802db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Анализ кластеров\n",
    "cluster_analysis = train_df.groupby('cluster').agg({\n",
    "                                    'price': ['mean', 'median', 'std', 'count'],\n",
    "                                    'power': 'mean',\n",
    "                                    'age': 'mean',\n",
    "                                    'kilometer': 'mean',\n",
    "                                    'brand': lambda x: x.mode()[0] if len(x.mode()) > 0 else 'mixed'\n",
    "}).round(0)\n",
    "\n",
    "cluster_analysis.columns = ['Price_mean', 'Price_median', 'Price_std', 'Count', 'Power_avg', 'Age_avg', 'Km_avg', 'Top_Brand']\n",
    "print(cluster_analysis.sort_values('Price_mean', ascending=False))\n",
    "\n",
    "# Сравним разброс\n",
    "print(f\"Общий std цены: {train_df['price'].std():.0f}€\")\n",
    "print(f\"Средний std внутри кластеров: {cluster_analysis['Price_std'].mean():.0f}€\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb24c8a",
   "metadata": {},
   "source": [
    "Если средний std внутри кластеров < общего std - значит, кластеризация работает."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9928e8",
   "metadata": {},
   "source": [
    "подправить код убрать лишнее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dff9f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Предположим, что train_df - ваш DataFrame с исходными данными\n",
    "# Выбираем нужные признаки для кластеризации\n",
    "cluster_features = ['power', 'kilometer', 'age', 'price']\n",
    "X_cluster = train_df[cluster_features].fillna(train_df[cluster_features].median())  # Заполняем пропуски медианой\n",
    "\n",
    "# Нормализация признаков перед кластеризацией (K-means чувствителен к масштабированию данных)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_cluster)\n",
    "\n",
    "# Кластеризация методом K-means\n",
    "n_clusters = 10  # Число кластеров\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Расчет среднего коэффициента Силуэта\n",
    "silhouette_avg = silhouette_score(X_scaled, labels)\n",
    "print(f'Средний коэффициент Силуэта: {silhouette_avg:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eb6d58",
   "metadata": {},
   "source": [
    "анализ кластеризации работает ли"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31f57b7",
   "metadata": {},
   "source": [
    "## **4.2. Стандартный Feature Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b1bd13",
   "metadata": {},
   "source": [
    "Всевозможные сложения и перемножения и деления  признаков качество модели lgbm не улучшили."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fb5812",
   "metadata": {},
   "source": [
    "# **5. Подготовка и обучение модели модели**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680e78cb",
   "metadata": {},
   "source": [
    "### **5.1. Отбор признаков с правильной корреляцией**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5b508d",
   "metadata": {},
   "source": [
    "Удалим признаки с мультиколинеарностью и низкой корреляцией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994d7556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Отбор по корреляции с таргетом\n",
    "target_corr = phik_corr['price'].abs()\n",
    "selected = target_corr[(target_corr >= 0.01) & (target_corr <= 0.9)].index.tolist()\n",
    "\n",
    "if 'price' in selected:\n",
    "    selected.remove('price')\n",
    "\n",
    "# 2. Сортируем по убыванию корреляции с таргетом (приоритет важным признакам)\n",
    "selected_sorted = sorted(selected, key=lambda x: target_corr[x], reverse=True)\n",
    "\n",
    "# 3. Удаляем мультиколлинеарность, сохраняя более важные\n",
    "corr_matrix = phik_corr.loc[selected, selected]\n",
    "to_drop = set()\n",
    "\n",
    "for i, col1 in enumerate(selected_sorted):      # проходит по признакам, отсортированным по важности (корреляции с таргетом)\n",
    "    if col1 in to_drop:                         # пропускает уже помеченные на удаление\n",
    "        continue\n",
    "    for col2 in selected_sorted[i+1:]:          # сравнивает каждый признак только с последующими (избегает дублирования) (индекс предыдущего +1)\n",
    "        if col2 in to_drop:                     # пропускает уже помеченные на удаление\n",
    "            continue\n",
    "        if corr_matrix.loc[col1, col2] > 0.9:   # если корреляция между признаками > 0.9\n",
    "            to_drop.add(col2)                   # # Удаляем col2 (у него корреляция с таргетом слабее)         \n",
    "\n",
    "selected_features = [f for f in selected_sorted if f not in to_drop]\n",
    "\n",
    "print(f\"Отобрано: {len(selected_features)}\")\n",
    "print(f\"Удалено: {len(to_drop)}\")\n",
    "print(f\"\\nТоп признаков по корреляции с таргетом:\")\n",
    "for f in selected_features:\n",
    "    print(f\"  {f:<20}: {target_corr[f]:.5f}\")\n",
    "\n",
    "# Классификация типов\n",
    "feature_types = {\n",
    "    'binary': [c for c in selected_features if train_df[c].nunique() == 2],\n",
    "    'ordinal': [c for c in selected_features if 3 <= train_df[c].nunique() <= 5 and train_df[c].dtype in ['int64', 'float64'] and train_df[c].apply(lambda x: x == int(x) if pd.notna(x) and isinstance(x, (int, float)) else True).all()],\n",
    "    'continuous': [c for c in selected_features if train_df[c].nunique() > 5],\n",
    "    'nominal': [c for c in selected_features if train_df[c].dtype == 'object' or (train_df[c].dtype in ['int64', 'float64'] and train_df[c].nunique() <= 5)]\n",
    "\n",
    "}\n",
    "\n",
    "for ftype, cols in feature_types.items():\n",
    "    print(f\"{ftype.capitalize()}: {cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241848a1",
   "metadata": {},
   "source": [
    "## **5.2. Классификация и стандартизация признаков**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b4c96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  классификация типов из отобранных признаков\n",
    "def create_preprocessor():\n",
    "    return ColumnTransformer([\n",
    "        ('std', RobustScaler(), [\n",
    "            'age', \n",
    "            'power', \n",
    "            'kilometer',\n",
    "            # 'region' \n",
    "            \n",
    "            ]), \n",
    "        ('cat', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), [\n",
    "            'model', \n",
    "            'vehicle_type', \n",
    "            'gearbox', # с ним lr лучше, lgbm хуже, без него lgbm лучше\n",
    "            'fuel_type', \n",
    "            'repaired', \n",
    "            'brand',\n",
    "            # 'cluster'\n",
    "            ])\n",
    "    ], remainder='drop')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88758c5e",
   "metadata": {},
   "source": [
    "**Для обучения модели рассмотрим две модели:**\n",
    "\n",
    "- **`Ridge`** - вместо более простого базового LinearRegression\n",
    "\n",
    "**Преимущества `Ridge`:**\n",
    "\n",
    "✅ Регуляризация — борется с переобучением\n",
    "\n",
    "✅ Устойчивость к мультиколлинеарности — когда признаки коррелируют\n",
    "\n",
    "✅ Стабильность — меньше чувствителен к выбросам\n",
    "\n",
    "✅ Лучше обобщает — особенно при большом количестве признаков\n",
    "\n",
    "\n",
    "- **`LightGBM`** - для примера градиентного бустинга.\n",
    "\n",
    "✅ В 10-20 раз быстрее XGBoost\n",
    "\n",
    "✅ Часто лучше CatBoost\n",
    "\n",
    "✅ Использует меньше RAM\n",
    "\n",
    "✅ Внутренний свой SelectKBest\n",
    "\n",
    "✅ Работает с категориями БЕЗ One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fba42f",
   "metadata": {},
   "source": [
    "## **5.3. Обучение модели**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a237bb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка данных\n",
    "X_train = train_df.drop('price', axis=1)\n",
    "y_train = train_df['price']\n",
    "\n",
    "X_test = test_df.drop('price', axis=1)\n",
    "y_test = test_df['price']\n",
    "\n",
    "X_val = val_df.drop('price', axis=1)\n",
    "y_val = val_df['price']\n",
    "\n",
    "# 1. Ridge регрессия \n",
    "pipeline_ridge = Pipeline([\n",
    "                            ('preprocessor', create_preprocessor()),\n",
    "                            ('model', Ridge(alpha=10, random_state=42))  # alpha сам отбирает важные признаки через кэфициенты\n",
    "])\n",
    "\n",
    "param_grid_ridge = {\n",
    "                    'model__alpha': [0.1, 1, 10, 100, 1000],  # Сила регуляризации\n",
    "                    'model__fit_intercept': [True, False],\n",
    "                    'model__solver': ['auto', 'svd', 'cholesky', 'lsqr']\n",
    "}\n",
    "\n",
    "grid_ridge_model = GridSearchCV(\n",
    "                                pipeline_ridge, \n",
    "                                param_grid_ridge, \n",
    "                                cv=KFold(n_splits=5, shuffle=True, random_state=42),\n",
    "                                scoring='neg_root_mean_squared_error', \n",
    "                                n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_ridge_model.fit(X_train, y_train)\n",
    "y_pred_ridge = grid_ridge_model.predict(X_val) # X_test заменили на X_val для оценки на валидационной выборке, так как тестовая используется только для финальной проверки модели и не должна влиять на выбор гиперпараметров.\n",
    "\n",
    "\n",
    "# Извлечение времени\n",
    "best_idx = grid_ridge_model.best_index_\n",
    "fit_time = grid_ridge_model.cv_results_['mean_fit_time'][best_idx]\n",
    "predict_time = grid_ridge_model.cv_results_['mean_score_time'][best_idx]\n",
    "\n",
    "rmse_ridge = np.sqrt(mean_squared_error(y_val, y_pred_ridge))  # y_test заменили на y_val для оценки на валидационной выборке, так как тестовая используется только для финальной проверки модели и не должна влиять на выбор гиперпараметров.\n",
    "\n",
    "print(f\"Ridge | RMSE: {rmse_ridge:.2f} € | Fit: {fit_time:.2f}s | Predict: {predict_time:.2f}s | Refit: {grid_ridge_model.refit_time_:.2f}s | Best params: {grid_ridge_model.best_params_}\")\n",
    "\n",
    "# 2. LightGBM \n",
    "pipe_lgb = Pipeline([\n",
    "                    ('preprocessor', create_preprocessor()),\n",
    "                    ('feature_selection', SelectFromModel(lgb.LGBMRegressor(n_estimators=50), threshold='median')),\n",
    "                    ('model', lgb.LGBMRegressor(random_state=42, verbose=-1)),\n",
    "\n",
    "])\n",
    "\n",
    "param_grid_lgb = {\n",
    "                'model__n_estimators': [200, 220, 230, 240, 250],\n",
    "                'model__max_depth': [10, 13],\n",
    "                'model__learning_rate': [0.1, 0.11],\n",
    "                'model__verbose': [-1],\n",
    "            }\n",
    "\n",
    "\n",
    "grid_lgbm_model = GridSearchCV(\n",
    "                                pipe_lgb, \n",
    "                                param_grid_lgb, \n",
    "                                cv=KFold(n_splits=5, shuffle=True, random_state=42),\n",
    "                                scoring='neg_root_mean_squared_error', \n",
    "                                n_jobs=-1,\n",
    "                                refit=True\n",
    "    )\n",
    "\n",
    "grid_lgbm_model.fit(X_train, y_train)\n",
    "y_pred_lgbm = grid_lgbm_model.predict(X_val)  # X_test заменили на X_val для оценки на валидационной выборке, так как тестовая используется только для финальной проверки модели \n",
    "\n",
    "# Извлечение времени\n",
    "best_idx = grid_lgbm_model.best_index_\n",
    "fit_time = grid_lgbm_model.cv_results_['mean_fit_time'][best_idx]\n",
    "predict_time = grid_lgbm_model.cv_results_['mean_score_time'][best_idx]\n",
    "\n",
    "\n",
    "rmse_lgbm = np.sqrt(mean_squared_error(y_val, y_pred_lgbm)) # y_test заменили на y_val для оценки на валидационной выборке, так как тестовая используется только для финальной проверки модели\n",
    "\n",
    "print(f\"LightGBM | RMSE: {rmse_lgbm:.2f} € | Fit: {fit_time:.2f}s | Predict: {predict_time:.2f}s | Refit: {grid_lgbm_model.refit_time_:.2f}s | Best params: {grid_lgbm_model.best_params_}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f2946b",
   "metadata": {},
   "source": [
    "Сохраним лучшую модель в переменную `best_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3221fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_lgbm_model.best_estimator_.named_steps['model'] if rmse_lgbm < rmse_ridge else grid_ridge_model.best_estimator_.named_steps['model']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0078ba",
   "metadata": {},
   "source": [
    "## **5.4. Извлечение лучших признаков из модели**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda487c2",
   "metadata": {},
   "source": [
    "Попробуем улучшить полученные результаты.\n",
    "\n",
    "Извлечем наиболее влиятельные признаки и попробуем переобучить модель только на них, если результат окажется хуже - откажемся от переобучяенной модели."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639346e7",
   "metadata": {},
   "source": [
    "Извлечем Feature Importance из лучшей модели чтобы сделать ее еще лучше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bec4e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Извлечение feature importance для лучшей модели (только для LightGBM, так как Ridge не предоставляет встроенный способ извлечения важности признаков, а его коэффициенты сложно интерпретировать из-за регуляризации)\n",
    "feature_names = grid_lgbm_model.best_estimator_.named_steps['preprocessor'].get_feature_names_out()\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': best_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nТоп-30 важных признаков:\")\n",
    "print(importance_df.head(30))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80fd803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация топ-30\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(importance_df.head(30)['feature'], importance_df.head(30)['importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Топ-30 важных признаков')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7d989c",
   "metadata": {},
   "source": [
    "Три наиболее важных признака:\n",
    "- возраст\n",
    "- мощность \n",
    "- пробег\n",
    "\n",
    "**Признаки с наименьшим влиянием на цену:**\n",
    "\n",
    "- частные модели каких-то брендов "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94763fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация bottom-30\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(importance_df.tail(30)['feature'], importance_df.tail(30)['importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Bottom-30 неважных признаков')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3687d81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Выбор топ-N признаков (например, топ-30)\n",
    "top_n = 30\n",
    "top_features = importance_df.head(top_n)['feature'].tolist()\n",
    "top_indices = [list(feature_names).index(f) for f in top_features]\n",
    "\n",
    "print(f\"\\nОтобрано {top_n} признаков\")\n",
    "\n",
    "# 4. Трансформация данных с отбором признаков\n",
    "X_train_transformed = grid_lgbm_model.best_estimator_.named_steps['preprocessor'].transform(X_train)\n",
    "X_test_transformed = grid_lgbm_model.best_estimator_.named_steps['preprocessor'].transform(X_test)\n",
    "\n",
    "X_train_selected = X_train_transformed[:, top_indices]\n",
    "X_test_selected = X_test_transformed[:, top_indices]\n",
    "\n",
    "# 5. Переобучение модели на отобранных признаках\n",
    "# Получить лучшую модель\n",
    "best_model = grid_lgbm_model.best_estimator_.named_steps['model']\n",
    "\n",
    "# Создать копию с теми же параметрами\n",
    "model_refit_temp = lgb.LGBMRegressor(**best_model.get_params())\n",
    "model_refit_temp.fit(X_train_selected, y_train)\n",
    "y_pred_selected = model_refit_temp.predict(X_test_selected)\n",
    "\n",
    "rmse_selected = np.sqrt(mean_squared_error(y_test, y_pred_selected))\n",
    "\n",
    "print(f\"\\nМодель с {top_n} признаками RMSE: {rmse_selected:.2f}\")\n",
    "print(f\"Разница: {rmse_selected - rmse_lgbm:+.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e563277",
   "metadata": {},
   "source": [
    "Модель с отобранным топом признаков оказалась хуже, поэтому не будем отбирать топ признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d861cf8f",
   "metadata": {},
   "source": [
    "## **5.5. Дообучение модели на извлеченных признаках**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f02b55",
   "metadata": {},
   "source": [
    "RMSE Показывает среднюю ошибку в евро"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe6799d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверим среднюю цену\n",
    "print(f\"Средняя цена: {y_train.mean():.2f}€\")\n",
    "print(f\"RMSE LightGBM: {rmse_lgbm:.2f}€ ({rmse_lgbm/y_train.mean()*100:.1f}% от средней)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcdefff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "# Предсказания\n",
    "y_pred_train = grid_lgbm_model.predict(X_train)\n",
    "y_pred_val = grid_lgbm_model.predict(X_val)\n",
    "y_pred_test = grid_lgbm_model.predict(X_test)\n",
    "\n",
    "# RMSE\n",
    "rmse_train = round(np.sqrt(mean_squared_error(y_train, y_pred_train)))\n",
    "rmse_val = round(np.sqrt(mean_squared_error(y_val, y_pred_val)))\n",
    "rmse_test =  round(np.sqrt(mean_squared_error(y_test, y_pred_test)))\n",
    "\n",
    "# Dummy\n",
    "dummy = DummyRegressor(strategy='mean')\n",
    "dummy.fit(X_train_selected, y_train)\n",
    "rmse_dummy = round(np.sqrt(mean_squared_error(y_test, dummy.predict(X_test_selected))))\n",
    "\n",
    "# Таблица\n",
    "results = pd.DataFrame({\n",
    "    'Выборка': ['Train', 'Valid', 'Test', 'Разница Train=>Test', 'Разница Train=>Test (%)'],\n",
    "    'LGBM': [rmse_train, rmse_val, rmse_test, f'{rmse_test - rmse_train:+.2f}€', f'{(rmse_test - rmse_train) / rmse_train * 100:+.2f}%'],\n",
    "    'Dummy': [rmse_dummy, rmse_dummy, rmse_dummy, 0, 0],\n",
    "\n",
    "})\n",
    "\n",
    "# Транспонируем таблицу\n",
    "transposed_results = results.T\n",
    "\n",
    "# Названия строк становятся названиями столбцов\n",
    "new_header = transposed_results.iloc[0]\n",
    "transposed_results.columns = new_header\n",
    "transposed_results.drop('Выборка', axis=0, inplace=True)\n",
    "\n",
    "# Вывод результата округленного до двух знаков после запятой\n",
    "print(transposed_results.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c544c84",
   "metadata": {},
   "source": [
    "# **6 Выводы**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf6557e",
   "metadata": {},
   "source": [
    "## **6.1 Основные выводы**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c116c7b",
   "metadata": {},
   "source": [
    "## **6.2. Общее заключение**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c675c13e",
   "metadata": {},
   "source": [
    "В рамках проекта была разработана модель для определения рыночной стоимости автомобилей с пробегом для сервиса «Не бит, не крашен». \n",
    "\n",
    "Перед нами стояла задача обучить две модели с метриками не более 2500 € по RMSE. \n",
    "\n",
    "Работа выполнена в соответствии с требованиями заказчика по качеству предсказания, скорости работы и времени обучения. Поставленная задача успешно выполнена."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0adad56",
   "metadata": {},
   "source": [
    "\n",
    "### **6.2.1. Выполненные этапы:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81d7856",
   "metadata": {},
   "source": [
    "1. Подготовка данных:\n",
    "\n",
    "- Загрузка датасета из 354 тыс. записей с 15 признаками\n",
    "\n",
    "- Разделение на \n",
    "\n",
    "    -- 60%: обучающую ( 212 620 строк);\n",
    "\n",
    "    -- 15%: валидационную (53 156) выборку.\n",
    "\n",
    "    -- 25%: тестовую (88 593 строк); \n",
    "\n",
    " \n",
    "\n",
    "- Создание и предварительная обработка в автоматизированном пайплайне из 7 последовательных шагов:\n",
    "\n",
    "- Коррекция ошибок (замена некорректного значения 0 в `RegistrationMonth` на медиану 6; замена `gasoline` на `petrol`);\n",
    "\n",
    "- Удаление неинформативных столбцов (`DateCrawled`, `DateCreated`, `LastSeen`, `NumberOfPictures`);\n",
    "\n",
    "- Проверка и Нормализация дробных чисел;\n",
    "\n",
    "- Оценка выбросов методом IQR, обработка выбросов методом винзоризации с удалением экстремальных значений (определены и оставлены только нормальные значения для `Power`: 1-1900 л.с., `RegistrationYear`: 1920-2016, `Price` > 1€);\n",
    "\n",
    "- Обработка пропущенных значений (подстановка `Unknown`);\n",
    "\n",
    "- Удаление явных дубликатов;\n",
    "\n",
    "- Проверка на наличие неявных дубликатов и проверка уникальных значений.\n",
    "\n",
    "Качественная предобработка данных принесла наибольший эффект для качественного обучения модели.\n",
    "\n",
    "Предобработка проводилась избирательно: все методы применялись к тренировочной выборке и только половина из них к тестовой и валидационной выборке. Выбросы удалялись в тестовой выборке только по цене, так как на проде целевая перемення на вход не приходит, ее надо предсказывать. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f90ebaa",
   "metadata": {},
   "source": [
    "### **6.2.2. Глубокий анализ данных (EDA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0d38e1",
   "metadata": {},
   "source": [
    "Ключевой особенностью проекта стал не просто механический подход к обучению моделей, а комплексный исследовательский анализ.\n",
    "\n",
    "- **Корреляционный анализ:** выявлены наиьболее влиятельные признаки, влияющие на цену автомобиля, устраненеа мультиколлинеарность с коэффициентом более 0.9. Все признаки с коэффициентом  менее 0.9 оказались важны, исключили дополнительно признак `PostalCode`.\n",
    "\n",
    "- **Поиск скрытых паттернов и взаимосвязей:**\n",
    "\n",
    "- Анализ однородности дисперсий: проведены тесты Левена, Бартлетта и Флигнера для обнаружения подозрительных паттернов и неоднородности данных\n",
    "\n",
    "- Проверка парадокса Симпсона: исключена возможность обманчивых корреляций из-за скрытых группировок\n",
    "\n",
    "- Комплексный анализ неоднородности:\n",
    "\n",
    "- Тесты однородности дисперсий\n",
    "\n",
    "- Анализ группировок по категориальным признакам\n",
    "\n",
    "- Проверка стабильности корреляций\n",
    "\n",
    "- Поиск скрытых кластеров\n",
    "\n",
    "- Анализ нелинейных связей\n",
    "\n",
    "Поиск скрытых взаимосвязей ожидаемой пользы для качества обучения модели не принес, но мы предприняли попытку обнаружить нелинейные скрытые взаимосвязи - таковых необнаружено."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fbc0c7",
   "metadata": {},
   "source": [
    "### **6.2.3. Feature Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537e802e",
   "metadata": {},
   "source": [
    "- Создали новые признаки: Возраст авто `'age'` вместо пробега `'Kilometer'` (лучше коррелирует), `'region'` на основе `'postal_code'`.\n",
    "\n",
    "- Отбор признаков с удалением мультиколлинеарных переменных\n",
    "\n",
    "- Классификация признаков по типу (числовые/категориальные)\n",
    "\n",
    "- Определение оптимального набора признаков для обучения\n",
    "\n",
    "- Попытки создания новых признаков путем сложения, перемножения и кластеризации, но  значительного улучшения не принесли)\n",
    "\n",
    "- Кластеризация данных и создания нового признака на основе кластеризации (ожидаемого эффекта кластеризация не принесла, да и модель LGBM сама внутри это делает)\n",
    "\n",
    "- При обучении в пайплайн зашщили `SelectFromModel`, которая была призвана отобрать лучшие признаки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c143f3",
   "metadata": {},
   "source": [
    "### **6.2.4. Обучение и выбор моделей**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4b1917",
   "metadata": {},
   "source": [
    "Протестированы две модели с использованием разработанного пайплайна:\n",
    "\n",
    "**1) Лучшей моделью мы определили LightGBM** на тесте\n",
    "\n",
    "**Модель: LightGBM** ⭐\n",
    "\n",
    "**RMSE:** 1481 €\n",
    "\n",
    "**Время обучения:** 6 сек\n",
    "\n",
    "**Лучшие параметры:** learning_rate=0.11, max_depth=13, n_estimators=250\n",
    "\n",
    "Проведена гиперпараметрическая оптимизация с использованием GridSearchCV и кросс-валидацией.\n",
    "\n",
    "**2) Вторая модель Ridge Regression** показала худшие результаты на валидационной выборке:\n",
    "\n",
    "**RMSE:** 2270 €\n",
    "\n",
    "**Время обучения:** 6 сек\n",
    "\n",
    "**Лучшие параметры**: fit_intercept=True, positive=False\n",
    "\n",
    "✅ Хотя обе модели выполнили требование RMSE < 2500 €\n",
    "\n",
    "✅ LightGBM показала лучшие результаты по всем критериям:\n",
    "\n",
    "- На 34% точнее Linear Regression (1466 vs 2225 €)\n",
    "\n",
    "- На 0.22 сек быстрее в обучении (6.12 vs 6.35 сек)\n",
    "\n",
    "Оптимальный баланс качества и скорости"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a80c67c",
   "metadata": {},
   "source": [
    "### **6.2.5. Оптимизация обученных моделей**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbeadea2",
   "metadata": {},
   "source": [
    "Предприняли попытку извлечения лучших признков, но это оказалось ни к чему, так как модель уже достигла максимальных результатов, поэтому переобучать модель не стали."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f79c86",
   "metadata": {},
   "source": [
    "### **6.2.6. Ключевые факторы успеха**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce73f34",
   "metadata": {},
   "source": [
    "\n",
    "**Таких высоких показателей удалось добиться благодаря в первую очередь:**\n",
    "\n",
    "- Качественной предобработке данных: прежде всего, тщательная обработка выбросов и пропущенных значений, замена ошибочных значений, ограничение максимальных и минимальных значений;\n",
    "\n",
    "- Подбор наиболее удачных вариантов обработки выбросов и пропусков;\n",
    "\n",
    "- Подбор гиперпараметров;\n",
    "\n",
    "- Глубокий EDA: выявление скрытых паттернов, анализ неоднородности, проверка статистических гипотез;\n",
    "\n",
    "- Систематический подход: создание воспроизводимого пайплайна с автоматической обработкой данных;\n",
    "\n",
    "- Комплексный анализ: не ограничились базовыми методами, провели многоуровневое исследование структуры данных.\n",
    "\n",
    "- Обучение нескольких моделей, выбирая лучшую;\n",
    "\n",
    "**Практическая ценность:**\n",
    "\n",
    "Разработанное решение обеспечивает:\n",
    "\n",
    "- Автоматическую обработку новых данных через пайплайн\n",
    "\n",
    "- Воспроизводимость результатов\n",
    "\n",
    "- Масштабируемость для больших объемов данных\n",
    "\n",
    "- Готовность к продакшену: модель готова к интеграции в приложение сервиса\n",
    "\n",
    "- Модель LightGBM рекомендуется к внедрению для быстрой и точной оценки рыночной стоимости автомобилей в приложении «Не бит, не крашен»."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
